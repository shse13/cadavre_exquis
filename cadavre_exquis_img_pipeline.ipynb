{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876295ee-5928-4a2f-bad6-5ebbb3b3df19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Using cached diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers) (8.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers) (3.13.1)\n",
      "Collecting huggingface-hub>=0.23.2 (from diffusers)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from diffusers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.32.3)\n",
      "Collecting safetensors>=0.3.1 (from diffusers)\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n",
      "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, diffusers, transformers\n",
      "Successfully installed diffusers-0.31.0 huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75deb2b7-dc65-492f-98c9-b6cbc7804233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARD CODED TO DELETE\n",
    "prompt = 'contrary subject hearing small law'\n",
    "#negative_prompt = 'blurry, text, letters, typography, writing, low resolution, watermark, patterns, distorted face, bad anatomy, uncanny, extra fingers, missing fingers, missing faces, pixelated, deformed'\n",
    "negative_prompt = 'blurry, text, letters, typography, writing, low resolution, watermark, patterns, distorted face, bad anatomy, uncanny, extra fingers, missing fingers, missing faces, pixelated, deformed, no text'\n",
    "positive_prompt = 'clean lines, well-balanced composition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef96ee88-05bd-4f44-87d7-0da47871416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = prompt.split()\n",
    "height = 1080/len(words)  # Set height\n",
    "width = 1080  # Set width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2db591c-1a7d-41eb-b025-5b182fc1f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import (\n",
    "    AutoPipelineForImage2Image,\n",
    "    ControlNetModel,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    ")\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers.utils import load_image, logging\n",
    "from diffusers.utils.logging import set_verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb46dc7-52a4-4091-b851-a4bd8dd31991",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c2c3b7-cf90-496e-bbac-6c9ee9c5b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel\n",
    "import torch\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"destitech/controlnet-inpaint-dreamer-sdxl\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20e71b9-df04-4e1c-a155-6383b4fc4557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f72ce7b66d4044b450afc2e9dbcb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionXLControlNetPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunDiffusion/Juggernaut-XL-v9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                               \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                               \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m                                                               \u001b[49m\u001b[43mcontrolnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:454\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[0;32m--> 454\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    457\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    461\u001b[0m ):\n\u001b[1;32m    462\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/diffusers/models/modeling_utils.py:1031\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         )\n\u001b[0;32m-> 1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\"RunDiffusion/Juggernaut-XL-v9\",\n",
    "                                                               torch_dtype=torch.float16,\n",
    "                                                               variant=\"fp16\",\n",
    "                                                               controlnet=controlnet).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090aeeb-b53a-415b-ad27-dc13eed2f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f495a6-8927-44b6-a7cd-4155702f1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler.config.use_karras_sigmas = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6180d-1f09-4828-a505-2c2eea57570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"sdxl_models\",\n",
    "    weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\",\n",
    "    image_encoder_folder=\"models/image_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fb1d0-00c5-4af9-a35f-329225832732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_ip_adapter_scale(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb567f0-f688-466c-ae05-1eb3ec172f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = Image.open(\"images_generated_to_clean2/expanded_only.jpg\").convert(\"RGB\")\n",
    "source_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e348d-ae08-470a-9f54-4c385845ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas_width, canvas_height = 1024, 512\n",
    "blank_canvas = Image.new(\"RGBA\", (canvas_width, canvas_height), \"WHITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401ea27-178a-464a-a931-63ce1dd4d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_offset = (canvas_width - source_image.width) // 4\n",
    "y_offset = (canvas_height - source_image.height) // 4\n",
    "print(f'x={x_offset}, y={y_offset}')\n",
    "blank_canvas.paste(source_image, (x_offset, y_offset))\n",
    "blank_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c469c-d23f-4008-aeb3-e2f7604f7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Image.new(\"L\", (canvas_width, canvas_height), 0) # L for luminance --> grayscale mode                 \n",
    "mask.paste(255, (x_offset, y_offset, x_offset + source_image.width, y_offset + source_image.height))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63541051-5bcc-4e78-bc42-c6774b28edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = IPAdapterMaskProcessor()\n",
    "ip_masks = processor.preprocess(mask, height=canvas_height, width=canvas_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad36e19-c5ac-4002-a01e-ca939a535428",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_masks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73d71eec-d23c-447e-b5fb-8a71a5d028aa",
   "metadata": {},
   "source": [
    "new_controlnet_image = Image.new(\"RGBA\", mask.size, \"WHITE\")\n",
    "new_controlnet_image.alpha_composite(mask)\n",
    "new_controlnet_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb66ee-3f54-4da2-8e3e-412eceaea52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f640e-1e4f-45e4-bf3f-4f7156fe35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pipeline(prompt=prompt + positive_prompt,\n",
    "                   height=canvas_height,\n",
    "                   width=canvas_width,\n",
    "                   guidance_scale=6.5,\n",
    "                   num_inference_steps=25,\n",
    "                   generator=generator,\n",
    "                   image=mask,\n",
    "                   controlnet_conditioning_scale=0.9,\n",
    "                   control_guidance_end=0.9,\n",
    "                   ip_adapter_image=blank_canvas,\n",
    "                   cross_attention_kwargs={\"ip_adapter_masks\": ip_masks},\n",
    "                   output_type=\"latent\").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566b573-1922-4001-a1ab-40cd49002bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57b437-1e59-4f4d-ba08-aeb2d94daa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline, controlnet=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8c287-f520-43a0-85cd-e2731be31c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadavre_exquis01 = pipeline_img2img(prompt=prompt+positive_prompt,\n",
    "                                    negative_prompt=negative_prompt,\n",
    "                                    guidance_scale=3.0,\n",
    "                                    num_inferences_steps=25,\n",
    "                                    generator=generator,\n",
    "                                    image=latents,\n",
    "                                    strength=0.2,\n",
    "                                    ip_adapter_image=blank_canvas,\n",
    "                                    cross_attention_kwargs={\"ip_adapter_masks\": ip_masks}).images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015b619-f62b-46c4-8000-ebf05b204bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadavre_exquis01.save(\"outpainting_exquis_01.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c9fc7-f390-49e3-84c8-90905a9a656d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af3889-e0aa-4c21-a1c3-d30273785eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7322b0-c777-4104-ac3d-df8e674195e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917ae8e-c80e-4bc1-9f09-7e7affc589a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f85234e-ddb8-4cca-9f58-82aef8c40193",
   "metadata": {},
   "source": [
    "# UTILISER https://huggingface.co/docs/diffusers/en/advanced_inference/outpaint\n",
    "https://huggingface.co/destitech/controlnet-inpaint-dreamer-sdxl\n",
    "\n",
    "https://github.com/huggingface/diffusers/discussions/7482\n",
    "\n",
    "https://huggingface.co/destitech/controlnet-inpaint-dreamer-sdxl\n",
    "\n",
    "Regénérer des plus longues prompts, éventuellement que des noms, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b37fb-9a8e-47b3-b0de-721a1ce9c3d8",
   "metadata": {},
   "source": [
    "# Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b185b666-4290-4494-a0e5-139b64ed304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "%reset -f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1454f3e7-147b-4497-9759-0da1cb85e8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 22:07:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A16                     On  | 00000000:27:00.0 Off |                    0 |\n",
      "|  0%   41C    P0              28W /  62W |  13918MiB / 15356MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd5d20-9924-4a92-baa5-4915e8a8568b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
