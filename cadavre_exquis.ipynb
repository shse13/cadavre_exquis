{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82c325c9-7220-4198-9cd9-c2fe6eaaa13c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.18.1+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: diffusers in /opt/conda/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers) (8.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from diffusers) (0.26.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "#!pip install spacy\n",
    "!python -m nltk.downloader averaged_perceptron_tagger\n",
    "!pip install --upgrade nltk\n",
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install --upgrade diffusers accelerate transformers\n",
    "# !pip install controlnet_aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35caaac9-40a2-4aaa-bb1b-6c5a2f2ea913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import gutenberg\n",
    "#from nltk import pos_tag\n",
    "import random\n",
    "#import spacy\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef5122-3a42-4e21-92fa-9294fcab9986",
   "metadata": {},
   "source": [
    "# PROMPT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcd6b1-5c0c-459c-88bd-9455282bfc56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b368b658-99cd-44c0-a6a3-df51b41cf5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet dataset\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913dcac4-d056-4d88-a736-86271ef9457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backups\n",
    "nouns = list(wn.all_synsets('n'))\n",
    "adjectives = list(wn.all_synsets('a'))\n",
    "verbs = list(wn.all_synsets('v'))\n",
    "random_noun = random.choice(nouns).lemmas()[0].name()\n",
    "random_adjective = random.choice(adjectives).lemmas()[0].name()\n",
    "random_verb = random.choice(verbs).lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825adeb-f29e-4e15-87fa-fc4f8fad79fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Random words from famous artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2beb7a8e-4d16-4fb8-a0b3-d46649af9141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943a8506-b81c-41aa-b13d-6da7079964fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_complete = gutenberg.words('austen-emma.txt')\n",
    "austen_emma = ' '.join([word for word in book_complete if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804d099f-70e0-4488-9e9d-e4d8f6ef6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append(\"/home/jovyan/nltk_data\")  # Ensure the correct path is used\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='/home/jovyan/nltk_data')\n",
    "nltk.download('punkt_tab')  # This will download the missing tokenizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc6def0-77c2-4eba-b2e9-42d166e44e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"vblagoje/bert-english-uncased-finetuned-pos\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e0e942-410b-4696-9a3b-d120d7960a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(text):\n",
    "\n",
    "    def chunk_text(text, max_length=512):\n",
    "        words = text.split()  # Split text into words\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "    \n",
    "        # Iterate over words and form chunks\n",
    "        for word in words:\n",
    "            if len(\" \".join(current_chunk + [word])) <= max_length:\n",
    "                current_chunk.append(word)\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                \n",
    "        # Add the last chunk if it exists\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        return chunks\n",
    "        \n",
    "    # Chunk the input text\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # Initialize lists to store words by POS\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    \n",
    "    # Process each chunk and classify POS\n",
    "    for chunk in chunks:\n",
    "        word_classification = pipe(chunk)\n",
    "        \n",
    "        for word in word_classification:\n",
    "            pos = word.get('entity')\n",
    "            word_text = word.get('word')\n",
    "            \n",
    "            if pos == 'NOUN':\n",
    "                nouns.append(word_text)\n",
    "            elif pos == 'VERB':\n",
    "                verbs.append(word_text)\n",
    "            elif pos == 'ADJ':\n",
    "                adjectives.append(word_text)\n",
    "                \n",
    "    # Filter out tokenized words starting with ##\n",
    "    nouns = [word for word in nouns if not word.startswith(\"##\")]\n",
    "    verbs = [word for word in verbs if not word.startswith(\"##\")]\n",
    "    adjectives = [word for word in adjectives if not word.startswith(\"##\")]\n",
    "\n",
    "    # Check if lists are non-empty before selecting random words\n",
    "    prompt_components = []\n",
    "    \n",
    "    if adjectives:\n",
    "        prompt_components.append(random.choice(adjectives))\n",
    "    if nouns:\n",
    "        prompt_components.append(random.choice(nouns))\n",
    "    if verbs:\n",
    "        prompt_components.append(random.choice(verbs))\n",
    "    if adjectives:\n",
    "        prompt_components.append(random.choice(adjectives))\n",
    "    if nouns:\n",
    "        prompt_components.append(random.choice(nouns))\n",
    "    \n",
    "    # Join the prompt components\n",
    "    prompt = ' '.join(prompt_components)\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d45d99-74b7-422f-b4ec-248a2636493e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contrary subject hearing small law'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(15)\n",
    "\n",
    "prompt = prompt_generation(austen_emma)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f677e3-596a-4f68-9c05-6146ae98fd8d",
   "metadata": {},
   "source": [
    "# IMAGE GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd88f7-3782-4457-bfc0-78574c247732",
   "metadata": {},
   "source": [
    "## N°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00139089-e11b-4222-a5b6-4495db4f1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionPipeline\n",
    "#from diffusers import StableDiffusionUpscalePipeline\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4fad0445-4fed-4a07-a5c5-b3c41a0ebb34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f44cb7a0f894a35bcbf2fb34185f989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.31.0\",\n",
       "  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generator = torch.Generator().manual_seed(42) # seed\n",
    "txt_2_img_pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n",
    "txt_2_img_pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b5836502-ab4e-48b9-ac6a-c6fbba9162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "75deb2b7-dc65-492f-98c9-b6cbc7804233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARD CODED TO DELETE\n",
    "prompt = 'contrary subject hearing small law'\n",
    "#negative_prompt = 'blurry, text, letters, typography, writing, low resolution, watermark, patterns, distorted face, bad anatomy, uncanny, extra fingers, missing fingers, missing faces, pixelated, deformed'\n",
    "negative_prompt = 'blurry, text, letters, typography, writing, low resolution, watermark, patterns, distorted face, bad anatomy, uncanny, extra fingers, missing fingers, missing faces, pixelated, deformed, no text'\n",
    "positive_prompt = 'clean lines, well-balanced composition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef96ee88-05bd-4f44-87d7-0da47871416f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3873cdf4784120a0d56eb12bb88e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrary\n"
     ]
    }
   ],
   "source": [
    "height = 1080/len(words)  # Set height\n",
    "width = 1080  # Set width\n",
    "\n",
    "words = prompt.split()\n",
    "image_1 = txt_2_img_pipe(words[0] + positive_prompt, negative_prompt=negative_prompt, guidance_scale=13, num_inference_steps= 80, height=height, width=width).images[0]\n",
    "image_1.save('image_1.png')\n",
    "\n",
    "# Show the image\n",
    "image_1\n",
    "print(words[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28b431-9f9c-4869-b907-d0c9079074af",
   "metadata": {},
   "source": [
    "## Extend picture n°2 and followings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6c6a8e6e-a3c8-4c3f-b245-408929fe5ae2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c931624816441939ed910ae66fecb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.31.0\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-2-1-base\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16)\n",
    "img2img_pipe.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60412ec5-b31f-4fb1-9d27-b23bf5ec4e40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GIF PREPARATION - MODIFY IMAGE WITH NEXT WORD. todo modify format if for instagram\n",
    "Input = previous image, next prompt word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db42cfd6-3177-4076-ba74-7b9ec7037b35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dedb0e8636443aba9635e3e2841887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_image_2.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2b6548fb714a7192c2d3829a3e72c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_image_3.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ff699b81dc4ffc9629553888a13e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_image_4.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3b3c3e26f5497386de86cc5429d109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_image_5.jpg\n"
     ]
    }
   ],
   "source": [
    "words = prompt.split()\n",
    "\n",
    "strength = 0.8  # How much the original image is preserved\n",
    "guidance_scale = 7 # How closely the image should match the prompt\n",
    "\n",
    "# Set the initial image as the starting point\n",
    "current_image = Image.open(\"image_1.png\").convert(\"RGB\")\n",
    "\n",
    "# Iterate over each word in the prompt\n",
    "for i, word in enumerate(words[1:], start=1):\n",
    "\n",
    "    output = img2img_pipe(\n",
    "        prompt=word + positive_prompt,  \n",
    "        negative_prompt=negative_prompt,\n",
    "        image=current_image, \n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale\n",
    "    ).images[0]\n",
    "\n",
    "    # Save the current output image\n",
    "    output_filename = f\"output_image_{i+1}.jpg\"\n",
    "    output.save(output_filename)\n",
    "    \n",
    "    # Use the current output image as the input for the next iteration\n",
    "    current_image = output\n",
    "\n",
    "    print(f\"Saved image: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea75573-4dee-44f2-82ba-51791383f82e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gif maker iterating with the entire prompt\n",
    "Could be with several different prompt as well and/or poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50440c5-7ba1-4a64-b9de-f33a60130ae8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0336cfc8e2fd4649b5903d30ca762dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_full_prompt_1.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed93ba37fcd24c6b97a264235f2e6d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_full_prompt_2.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d278c63ce10441eab23caaad179e59b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_full_prompt_3.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb65b3b1ef64c2cbd9b6c7077d4898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_full_prompt_4.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a19e807a1a4bdf86d593d5d3412654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: output_full_prompt_5.jpg\n"
     ]
    }
   ],
   "source": [
    "words = prompt.split()\n",
    "\n",
    "strength = 0.8  # How much the original image is preserved\n",
    "guidance_scale = 7 # How closely the image should match the prompt\n",
    "\n",
    "# Set the initial image as the starting point\n",
    "current_image = Image.open(\"image_1.png\").convert(\"RGB\")\n",
    "\n",
    "# Iterate over each word in the prompt\n",
    "for i in range(len(words)):\n",
    "\n",
    "    output = img2img_pipe(\n",
    "        prompt=prompt + positive_prompt,  \n",
    "        negative_prompt=negative_prompt,\n",
    "        image=current_image, \n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale\n",
    "    ).images[0]\n",
    "\n",
    "    # Save the current output image\n",
    "    output_filename = f\"output_full_prompt_{i+1}.jpg\"\n",
    "    output.save(output_filename)\n",
    "    \n",
    "    # Use the current output image as the input for the next iteration\n",
    "    current_image = output\n",
    "\n",
    "    print(f\"Saved image: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34138525-40c5-42fb-a59e-8336ca3d6716",
   "metadata": {},
   "source": [
    "### EXTEND IMAGE WITH NEXT WORD.\n",
    "Input = previous image, next prompt word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7f825c6d-8b88-47c8-a284-6c6eccbd3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_img(input_img, word):\n",
    "    input_img = Image.open(input_img).convert(\"RGB\")\n",
    "    expanded_size = ( input_img.width, input_img.height)\n",
    "    top_part = input_img.crop((0, 0, input_img.width, input_img.height // 5))\n",
    "    top_part = top_part.resize(expanded_size)\n",
    "\n",
    "    expanded_image = img2img_pipe(\n",
    "    prompt=word + positive_prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=top_part,\n",
    "    strength=0.75,\n",
    "    guidance_scale=7.5).images[0]\n",
    "    return expanded_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "842227a4-7cf1-405d-921d-4cd1fa24c033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ca7b6f79374bd0a7f1bac96b641716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0783516140b14ff38eeb00c8c000ccb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe75748a04a4b4fb931a9ff1e2607c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da472c40e9a4e7d968c739fb2ab39e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_img = 'image_1.png'\n",
    "for i in words[1:]:\n",
    "    expanded = expand_img(input_img, i)\n",
    "    expanded.save(f\"expanded_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b478569-c1db-43f8-a7ce-402871462815",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9084af4c1d044468b00c8e947279d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "abdcfa2d-9fc0-4f70-b2f1-07b5eacfdbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contrary', 'subject', 'hearing', 'small', 'law']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = prompt.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "405433a5-80b9-4895-a8c1-6c2095ee3853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9194d799d862494d8aad547e123a6e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3286a3b4dbf24247b7eaa04adc2f63d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6b1b11474c43d3b29787e8ec6c952a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5de687e7c540538c0f5d90f9b7af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938f8dc5956e4323bffd25709d9a04c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Load your input image\n",
    "original_image = Image.open(\"image_1.png\").convert(\"RGB\")\n",
    "\n",
    "# Iteratively expand the image upward\n",
    "for idx, word in enumerate(words):\n",
    "    # Define the new canvas size (extend height upward)\n",
    "    new_height = original_image.height + 256  # Add 256 pixels at the top\n",
    "    new_width = original_image.width\n",
    "    canvas = Image.new(\"RGB\", (new_width, new_height), (255, 255, 255))  # Create larger canvas\n",
    "    canvas.paste(original_image, (0, 256))  # Place the current image at the bottom\n",
    "\n",
    "    # Sample content from the top of the original image to guide blending\n",
    "    top_sample = original_image.crop((0, 0, new_width, 50))  # Take a slice from the top\n",
    "\n",
    "    # Create a mask for the blank area (the top part of the canvas)\n",
    "    mask = Image.new(\"L\", (new_width, new_height), 0)  # Black (preserve original)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.rectangle([0, 0, new_width, 256], fill=255)  # White (generate area at top)\n",
    "\n",
    "    # Use the current image and sampled content as the base for generation\n",
    "    prompt = f\"{word} blended with sky and texture from image\"\n",
    "    blended_prompt = f\"{word} inspired by the continuation of the scene\"\n",
    "\n",
    "    # Generate new content for the blank area\n",
    "    result = inpaint_pipe(\n",
    "        prompt=prompt + positive_prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=canvas,\n",
    "        mask_image=mask,\n",
    "        strength=0.75,  # Lower strength to preserve context\n",
    "        guidance_scale=7.5\n",
    "    ).images[0]\n",
    "\n",
    "    # Save intermediate results (optional)\n",
    "    intermediate_filename = f\"expanded_upward_step_{idx + 1}.jpg\"\n",
    "    result.save(intermediate_filename)\n",
    "\n",
    "    # Update the original image for the next iteration with the new expanded image\n",
    "    original_image = result  # Now use the expanded result as the base for the next step\n",
    "\n",
    "# Save the final expanded image\n",
    "original_image.save(\"final_expanded_upward_image3.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bec9a85-647a-47d8-b72f-0280936c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an image from an image dataset rather than generating it? and make a collage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78b11255-0e01-4673-b091-3f8268169cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETERNAL RIGHT SCROLL OR BOTTOM, ETC.\n",
    "# Prompt[:1] --> image1 --> right part of image1 + prompt[1:2] + style of image1 and noise/random --> image2 --> right part of image2 + prompt[2:3] + style of image 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecdfc2-cc52-4ad5-b172-86b3fcaa314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOOM OUT OF ZOOM IN\n",
    "# prompt[:1] --> image1 --> image1 + zoom out (bigger picture) --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5bb132ad-07c1-4baa-95d7-527abb9208b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc51cef41f95414d83dbf4c5ab70fe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b185b666-4290-4494-a0e5-139b64ed304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428e7a6-ca24-4407-8626-88b05b4b0a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
